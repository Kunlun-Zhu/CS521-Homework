{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Interval Bound Propagation, Programming). In this problem, you will \n",
    "\n",
    "implement interval bound propagation (IBP) training for a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize()\n",
       "  (1): Net(\n",
       "    (f1): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (f2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (f3): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (out): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "## Dataloaders\n",
    "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "## Simple NN. You can change this if you want. If you change it, mention the architectural details in your report.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.f1 = nn.Linear(28*28, 50)\n",
    "        self.f2 = nn.Linear(50, 50)\n",
    "        self.f3 = nn.Linear(50, 50)\n",
    "        self.out = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f2(x))\n",
    "        x = F.relu(self.f3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x - 0.1307)/0.3081\n",
    "\n",
    "# Add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_analysis_vectorized(model, x, eps):\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    f1_weight = model[1].f1.weight\n",
    "    f1_bias = model[1].f1.bias\n",
    "    f2_weight = model[1].f2.weight\n",
    "    f2_bias = model[1].f2.bias\n",
    "    f3_weight = model[1].f3.weight\n",
    "    f3_bias = model[1].f3.bias\n",
    "    out_weight = model[1].out.weight\n",
    "    out_bias = model[1].out.bias\n",
    "\n",
    "    x = x.view(x.size(0), -1)  \n",
    "    x_lower = x - eps\n",
    "    x_upper = x + eps\n",
    "\n",
    "    # Layer 1\n",
    "    W_pos = torch.clamp(f1_weight, min=0)\n",
    "    W_neg = torch.clamp(f1_weight, max=0)\n",
    "    f1_lower = torch.matmul(x_lower, W_pos.t()) + torch.matmul(x_upper, W_neg.t()) + f1_bias\n",
    "    f1_upper = torch.matmul(x_upper, W_pos.t()) + torch.matmul(x_lower, W_neg.t()) + f1_bias\n",
    "    f1_lower = F.relu(f1_lower)\n",
    "    f1_upper = F.relu(f1_upper)\n",
    "\n",
    "    # Layer 2\n",
    "    W_pos = torch.clamp(f2_weight, min=0)\n",
    "    W_neg = torch.clamp(f2_weight, max=0)\n",
    "    f2_lower = torch.matmul(f1_lower, W_pos.t()) + torch.matmul(f1_upper, W_neg.t()) + f2_bias\n",
    "    f2_upper = torch.matmul(f1_upper, W_pos.t()) + torch.matmul(f1_lower, W_neg.t()) + f2_bias\n",
    "    f2_lower = F.relu(f2_lower)\n",
    "    f2_upper = F.relu(f2_upper)\n",
    "\n",
    "    # Layer 3\n",
    "    W_pos = torch.clamp(f3_weight, min=0)\n",
    "    W_neg = torch.clamp(f3_weight, max=0)\n",
    "    f3_lower = torch.matmul(f2_lower, W_pos.t()) + torch.matmul(f2_upper, W_neg.t()) + f3_bias\n",
    "    f3_upper = torch.matmul(f2_upper, W_pos.t()) + torch.matmul(f2_lower, W_neg.t()) + f3_bias\n",
    "    f3_lower = F.relu(f3_lower)\n",
    "    f3_upper = F.relu(f3_upper)\n",
    "\n",
    "    # Output Layer\n",
    "    W_pos = torch.clamp(out_weight, min=0)\n",
    "    W_neg = torch.clamp(out_weight, max=0)\n",
    "    out_lower = torch.matmul(f3_lower, W_pos.t()) + torch.matmul(f3_upper, W_neg.t()) + out_bias\n",
    "    out_upper = torch.matmul(f3_upper, W_pos.t()) + torch.matmul(f3_lower, W_neg.t()) + out_bias\n",
    "\n",
    "    return out_lower, out_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_ibp(model, train_loader,\n",
    "             num_epochs=20, learning_rate=0.01, momentum=0.9,\n",
    "             initial_kappa=1.0, final_kappa=0.5,\n",
    "             initial_epsilon=0.0, final_epsilon=0.1):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    kappa = initial_kappa\n",
    "    decay = (initial_kappa - final_kappa) / num_epochs\n",
    "    epsilon = initial_epsilon\n",
    "    epsilon_increment = (final_epsilon - initial_epsilon) / num_epochs\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        kappa = max(kappa - decay, final_kappa)\n",
    "        epsilon = min(epsilon + epsilon_increment, final_epsilon)\n",
    "        \n",
    "        for data, target in tqdm(train_loader, desc=f'IBP Training Epoch {epoch}/{num_epochs}'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            ce_loss = nn.CrossEntropyLoss()(outputs, target)\n",
    "            out_lower, out_upper = interval_analysis_vectorized(model, data, epsilon)\n",
    "            z_hat = out_lower\n",
    "            z_hat[target] = out_upper[target]\n",
    "            ce_loss_hat = nn.CrossEntropyLoss()(z_hat, target)\n",
    "            total_loss = kappa * ce_loss + (1 - kappa) * ce_loss_hat\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, kappa: {kappa:.4f}, epsilon: {epsilon:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'IBP Training completed in {training_time:.2f} seconds')\n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_standard(model, train_loader, num_epochs=20, learning_rate=0.01, momentum=0.9):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in tqdm(train_loader, desc=f'Standard Training Epoch {epoch+1}/{num_epochs}'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Standard Training completed in {training_time:.2f} seconds')\n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_standard_accuracy(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Evaluating Standard Accuracy'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f'Standard Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_untargeted(model, x, y, k=10, eps=0.1, eps_step=0.01):\n",
    "\n",
    "    model.eval()\n",
    "    x_prime = x.clone().detach()\n",
    "    x_prime += 2 * eps * (torch.rand_like(x_prime) - 0.5)\n",
    "    x_prime = torch.clamp(x_prime, torch.clamp(x - eps, 0, 1), torch.clamp(x + eps, 0, 1))\n",
    "\n",
    "    for _ in range(k):\n",
    "        x_prime.requires_grad = True\n",
    "        output = model(x_prime)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        gradient = x_prime.grad.data\n",
    "        x_prime = x_prime + eps_step * gradient.sign()\n",
    "        x_prime = torch.max(torch.min(x_prime, x + eps), x - eps)\n",
    "        x_prime = torch.clamp(x_prime, 0, 1).detach()\n",
    "\n",
    "    return x_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robust_accuracy(model, test_loader, epsilon=0.1, alpha=0.02, iters=40):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in tqdm(test_loader, desc='Evaluating Robust Accuracy'):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        adv_data = pgd_untargeted(model, data, target, eps=epsilon, k=iters, eps_step=alpha)\n",
    "        outputs = model(adv_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f'Robust Accuracy (PGD, epsilon={epsilon}): {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IBP Training Epoch 1/10:   0%|          | 0/938 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'margin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_ibp \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(Normalize(), Net())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m model_ibp\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m ibp_training_time \u001b[38;5;241m=\u001b[39m train_ibp(model_ibp, train_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, \n\u001b[1;32m      5\u001b[0m                               initial_kappa\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, final_kappa\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, initial_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, final_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mtrain_ibp\u001b[0;34m(model, train_loader, num_epochs, learning_rate, momentum, initial_kappa, final_kappa, initial_epsilon, final_epsilon)\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m ibp_loss_fn(data, outputs, target)  \u001b[38;5;66;03m# Pass 'data' to loss function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 41\u001b[0m, in \u001b[0;36mIBPLoss.forward\u001b[0;34m(self, data, outputs, targets)\u001b[0m\n\u001b[1;32m     37\u001b[0m     max_other \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(other_upper, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#margin = true_low - max_other\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     robust_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;241m-\u001b[39mmargin \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     43\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkappa \u001b[38;5;241m*\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkappa) \u001b[38;5;241m*\u001b[39m robust_loss\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "\u001b[0;31mNameError\u001b[0m: name 'margin' is not defined"
     ]
    }
   ],
   "source": [
    "model_ibp = nn.Sequential(Normalize(), Net()).to(device)\n",
    "model_ibp.train()\n",
    "\n",
    "ibp_training_time = train_ibp(model_ibp, train_loader, num_epochs=10, learning_rate=0.01, momentum=0.9, \n",
    "                              initial_kappa=1.0, final_kappa=0.5, initial_epsilon=0.0, final_epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 1/10: 100%|██████████| 938/938 [00:17<00:00, 54.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 2/10: 100%|██████████| 938/938 [00:11<00:00, 82.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.1469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 3/10: 100%|██████████| 938/938 [00:10<00:00, 85.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.1082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 4/10: 100%|██████████| 938/938 [00:10<00:00, 86.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.0892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 5/10: 100%|██████████| 938/938 [00:10<00:00, 89.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 6/10: 100%|██████████| 938/938 [00:16<00:00, 58.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.0670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 7/10: 100%|██████████| 938/938 [00:17<00:00, 54.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 8/10: 100%|██████████| 938/938 [00:15<00:00, 59.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.0524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 9/10: 100%|██████████| 938/938 [00:24<00:00, 38.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Training Epoch 10/10: 100%|██████████| 938/938 [00:24<00:00, 37.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.0404\n",
      "Standard Training completed in 159.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_standard = nn.Sequential(Normalize(), Net()).to(device)\n",
    "\n",
    "model_standard.train()\n",
    "\n",
    "standard_training_time = train_standard(model_standard, train_loader, num_epochs=10, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating IBP Trained Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Standard Accuracy:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Standard Accuracy: 100%|██████████| 10000/10000 [00:01<00:00, 6983.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Accuracy: 97.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Robust Accuracy: 100%|██████████| 10000/10000 [01:44<00:00, 95.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust Accuracy (PGD, epsilon=0.1): 8.22%\n",
      "Evaluating Standard Trained Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Standard Accuracy: 100%|██████████| 10000/10000 [00:01<00:00, 8231.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Accuracy: 96.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Robust Accuracy:  35%|███▍      | 3454/10000 [00:36<01:09, 94.14it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Standard Trained Model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m standard_standard_acc \u001b[38;5;241m=\u001b[39m evaluate_standard_accuracy(model_standard, test_loader)\n\u001b[0;32m----> 7\u001b[0m standard_robust_acc \u001b[38;5;241m=\u001b[39m evaluate_robust_accuracy(model_standard, test_loader, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m, iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m, in \u001b[0;36mevaluate_robust_accuracy\u001b[0;34m(model, test_loader, epsilon, alpha, iters)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating Robust Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      6\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     adv_data \u001b[38;5;241m=\u001b[39m pgd_untargeted(model, data, target, eps\u001b[38;5;241m=\u001b[39mepsilon, k\u001b[38;5;241m=\u001b[39miters, eps_step\u001b[38;5;241m=\u001b[39malpha)\n\u001b[1;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(adv_data)\n\u001b[1;32m      9\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mpgd_untargeted\u001b[0;34m(model, x, y, k, eps, eps_step)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, y)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m gradient \u001b[38;5;241m=\u001b[39m x_prime\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     15\u001b[0m x_prime \u001b[38;5;241m=\u001b[39m x_prime \u001b[38;5;241m+\u001b[39m eps_step \u001b[38;5;241m*\u001b[39m gradient\u001b[38;5;241m.\u001b[39msign()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Evaluating IBP Trained Model:\")\n",
    "ibp_standard_acc = evaluate_standard_accuracy(model_ibp, test_loader)\n",
    "ibp_robust_acc = evaluate_robust_accuracy(model_ibp, test_loader, epsilon=0.1, alpha=0.02, iters=40)\n",
    "\n",
    "print(\"Evaluating Standard Trained Model:\")\n",
    "standard_standard_acc = evaluate_standard_accuracy(model_standard, test_loader)\n",
    "standard_robust_acc = evaluate_robust_accuracy(model_standard, test_loader, epsilon=0.1, alpha=0.02, iters=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTraining Time Comparison:\")\n",
    "print(f\"IBP Training Time: {ibp_training_time:.2f} seconds\")\n",
    "print(f\"Standard Training Time: {standard_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_tests = np.linspace(0.01, 0.1, 10)\n",
    "\n",
    "model_ibp.eval()\n",
    "verified_counts = {epsilon: 0 for epsilon in epsilon_tests}\n",
    "total = 0\n",
    "\n",
    "\n",
    "for data, target in tqdm(test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    total += 1\n",
    "    out_lower, out_upper = interval_analysis_vectorized(model_ibp, data, 0.0) \n",
    "    _, predicted = torch.max(out_lower, 1)\n",
    "    true_class = target.item()\n",
    "\n",
    "    for epsilon in epsilon_tests:\n",
    "        out_lower_eps, out_upper_eps = interval_analysis_vectorized(model_ibp, data, epsilon)\n",
    "        \n",
    "        true_low = out_lower_eps[true_class]\n",
    "        other_upper = out_upper_eps.clone()\n",
    "        other_upper[true_class] = -float('inf')\n",
    "        \n",
    "        if true_low > torch.max(other_upper):\n",
    "            verified_counts[epsilon] += 1\n",
    "\n",
    "verified_accuracies = {epsilon: 100.0 * count / total for epsilon, count in verified_counts.items()}\n",
    "\n",
    "\n",
    "# Plot verified accuracy vs epsilon\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epsilon_tests, [verified_accuracies[e] for e in epsilon_tests], marker='o')\n",
    "plt.title('Verified Accuracy vs Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Verified Accuracy (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print verified accuracies\n",
    "for epsilon in epsilon_tests:\n",
    "    print(f\"Epsilon: {epsilon:.2f}, Verified Accuracy: {verified_accuracies[epsilon]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adversarial_examples(model, test_loader, epsilon_tests):\n",
    "    model.eval()\n",
    "    adversarial_examples = {epsilon: [] for epsilon in epsilon_tests}\n",
    "    for data, target in tqdm(test_loader, desc='Finding Adversarial Examples'):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        true_class = target.item()\n",
    "        for epsilon in epsilon_tests:\n",
    "            out_lower, out_upper = interval_analysis_vectorized(model, data, epsilon)\n",
    "            true_lower = out_lower[0, true_class]\n",
    "            other_upper = out_upper.clone()\n",
    "            other_upper[0, true_class] = -float('inf')\n",
    "            if not (true_lower > other_upper).all():\n",
    "                adv_data = pgd_untargeted(model, data, target)\n",
    "                adversarial_examples[epsilon].append((data.cpu(), adv_data.cpu(), target.cpu()))\n",
    "                if len(adversarial_examples[epsilon]) >= 5:\n",
    "                    break\n",
    "    return adversarial_examples\n",
    "epsilon_tests = np.linspace(0.01, 0.1, 10)\n",
    "adversarial_examples = find_adversarial_examples(model_ibp, test_loader, epsilon_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adversarial examples for a specific epsilon\n",
    "epsilon_tests = np.linspace(0.01, 0.1, 10)\n",
    "\n",
    "for selected_epsilon in epsilon_tests:\n",
    "    if adversarial_examples:\n",
    "        num_examples = min(2, len(adversarial_examples))\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        print(f\"Adversarial Examples for epsilon={selected_epsilon}\")\n",
    "        print(\"num_examples\", num_examples)\n",
    "        print(len(adversarial_examples))\n",
    "        for i in range(num_examples):\n",
    "            orig, adv, target = adversarial_examples[selected_epsilon][i]\n",
    "            orig_img = orig.squeeze().numpy()\n",
    "            adv_img = adv.squeeze().numpy()\n",
    "            \n",
    "            plt.subplot(num_examples, 2, 2*i+1)\n",
    "            plt.imshow(orig_img, cmap='gray')\n",
    "            plt.title(f'Original: {target.item()}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(num_examples, 2, 2*i+2)\n",
    "            plt.imshow(adv_img, cmap='gray')\n",
    "            plt.title('Adversarial')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No adversarial examples found for epsilon={selected_epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See colab link in the home work file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
